{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "# Get an instance of StanfordCoreNLP by connecting to the server\n",
    "nlp = StanfordCoreNLP('http://jupyterlab-nfs-corenlp', port=9000)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = nltk.corpus.treebank.tagged_sents()[57]\n",
    "#sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The three ex-field staffers, Alexis Sklair, Nathaniel Brown, and Sterling Rettke, filed suit in federal court in New York City, \\\n",
    "     contending that field organizers were fraudulently induced to accept jobs with the Bloomberg campaign based on the promise of \\\n",
    "     guaranteed salaries through November 30, 2020.\"\n",
    "sent = nltk.pos_tag(nltk.word_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get named entities from NLTK\n",
    "res = nltk.ne_chunk(sent)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from code at http://www.nltk.org/book/ch07.html to return data rather than just output it\n",
    "# by Sal Barbosa\n",
    "# This function recursively traverses a tree, collecting information into a list reference\n",
    "# Input: A tree, t, a list, and a sublist (defaults to the empty list if one is not supplied)\n",
    "# Output: Modifies the original list passed in as a reference (lst)\n",
    "def traverse_ne(t,lst,sublst=[]):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        #print(t,type(t), end=\" \")\n",
    "        lst.append(t)\n",
    "    else:\n",
    "        # Now we know that t.node is defined\n",
    "        x = len(lst)\n",
    "        if t.label() != \"S\":sublst = [t.label()]\n",
    "        lst.append(sublst)      \n",
    "        for child in t:\n",
    "            traverse_ne(child,lst[x],lst[x])\n",
    "        lst[x] = tuple(lst[x])\n",
    "        #if t.label() != \"S\": sublst.insert(0, t.label())\n",
    "        #print('(', t.label(), end=\" \")\n",
    "        #print(')', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylst = []\n",
    "traverse_ne(res,mylst)\n",
    "nes = mylst[0]\n",
    "print(nes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output only named entities\n",
    "for itm in nes:\n",
    "    #print(itm)\n",
    "    if isinstance(itm[1],tuple):\n",
    "        print(itm[1:],\"is a\",itm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Relation Extraction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from code at http://www.nltk.org/book/ch07.html\n",
    "# using regular expressions to extract relationships between named entity types \n",
    "pat = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "#pat = re.compile(r'.*\\bof\\b')\n",
    "#pat = re.compile(r'.*\\band\\b')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = pat):\n",
    "    #for rel in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='ieer', pattern = pat):\n",
    "    #for rel in nltk.sem.extract_rels('PER', 'PER', doc, corpus='ieer', pattern = pat):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tagging (Named Entity and Part-of-Speech tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sal Barbosa\n",
    "# This function custom tags a sentence, using named entity types where applicable, and part-of-speech tags otherwise\n",
    "# Purpose: It may be used to construct chunk grammars that identify phrases based on named entities (vs. POS tags)\n",
    "# Input: It accepts a list of dictionaries output from the CoreNLP annotator with named entity tags as a property\n",
    "#        and has an optional parameter indicating whether the token, or its lemma should be returned (using lemmas defaults to False )\n",
    "# Output: It returns a list of tuples containing the token (or its lemma) and the named entity or part-of-speech tag\n",
    "def blend_pos_nes(d,uselemma=False):\n",
    "    toks = []                                           # will hold a list of tuples (token or lemma, ne or pos tag)\n",
    "    fullne = \"\"                                         # holds combined compound named entities, connecte with underscore (i.e. \"New_York\")\n",
    "    lastne = \"\"                                         # holds last named entity tage read - used to combine compound named entities\n",
    "    for t in d:                                         # t is a dictionary for each toekn in the sentence\n",
    "        if uselemma: tok = t['lemma']\n",
    "        else: tok = t['originalText']\n",
    "        pos = t['pos']\n",
    "        ne = t['ner']\n",
    "        if ne == 'O':                                   # this token is not (part of) a named entity\n",
    "            if lastne != \"\":                            # if previously \"inside\" a compound named entity (like 'New York'), terminate it\n",
    "                toks.append((fullne.strip('_'),lastne)) # and add the token and its ne tag to the list\n",
    "                lastne = \"\"                             # reset lastne and fullne\n",
    "                fullne = \"\"                             # to empty strings\n",
    "            toks.append((tok,pos))                  # not previously inside ne, so add token/lemma and pos tag\n",
    "        else:                                           # this token is (part of) a named entity\n",
    "            if ne == lastne:                            # if inside a compound named entity of the same type\n",
    "                fullne += '_' + tok                     # keep building it\n",
    "            else:                                       # otherwise this is a new named entity \n",
    "                if lastne != \"\":                            # if previously \"inside\" a different named entity than this one\n",
    "                    toks.append((fullne.strip('_'),lastne)) # close it out and add the token and its ne tag to the list              \n",
    "                lastne = ne                             # set lastne to the ne of this token\n",
    "                fullne = tok                            # begin (a possibly compound) token with this token/lemma\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sal Barbosa\n",
    "# This is a helper function that runs the NE annotator, unpacks the json return, invokes blend_pos_ne, and returns the custom tagging\n",
    "def run_ne(s):\n",
    "    props = {'annotators': 'ner','outputFormat':'json'} # set annotator to provide named entities and return as json (otherwise it's a string)\n",
    "    nes2 = nlp.annotate(s, properties=props)   # apply the annotator: results are in json format\n",
    "    d = json.loads(nes2)['sentences'][0]['tokens']\n",
    "    #print(d)\n",
    "    tagged = blend_pos_nes(d)\n",
    "    return tagged    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom tagging output\n",
    "print(s)\n",
    "props = {'annotators': 'ner','outputFormat':'json'} # set annotator to provide named entities and return as json (otherwise it's a string)\n",
    "nes2 = nlp.annotate(s, properties=props)   # apply the annotator: results are in json format\n",
    "d = json.loads(nes2)['sentences'][0]['tokens']\n",
    "tagged = blend_pos_nes(d)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = run_ne(s)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom tags in a chunk grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk grammar for basic noun phrases and verb phrases\n",
    "# the second line in the grammar accepts persons in text like \"The eccentric Albert Einstein\" or simply \"George Washington\" as noun phrases\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT>?<ORDINAL>?<JJ>*<NN|NNS>+}\n",
    "      {<DT>?<JJ>*<PERSON>}\n",
    "  PP: {<IN|TO><NP>} \n",
    "  V:  {<VB|VBD|VBG|VBN|VBP|VBZ|>}\n",
    "  VP: {<V><NP><PP>*<NP>*}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of use of grammar's output with custom tagging \n",
    "s1 = 'The eccentric Albert Einstein once said that imagination is more important than knowledge.'\n",
    "#print(nltk.pos_tag(nltk.word_tokenize(s1)))\n",
    "tagged = run_ne(s1)\n",
    "print(tagged)\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(tagged))\n",
    "#s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"The second horse ran fast.\"\n",
    "nes2 = nlp.annotate(line, properties=props)   # apply the annotator: results are in json format\n",
    "d = json.loads(nes2)['sentences'][0]['tokens']\n",
    "tagged = blend_pos_nes(d)\n",
    "print(cp.parse(tagged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
