{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "# Get an instance of StanfordCoreNLP by connecting to the server\n",
    "nlp = StanfordCoreNLP #('http://jupyterlab-nfs-corenlp', port=9000)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = nltk.corpus.treebank.tagged_sents()[57]\n",
    "#sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The three ex-field staffers, Alexis Sklair, Nathaniel Brown, and Sterling Rettke, filed suit in federal court in New York City, \\\n",
    "     contending that field organizers were fraudulently induced to accept jobs with the Bloomberg campaign based on the promise of \\\n",
    "     guaranteed salaries through November 30, 2020.\"\n",
    "sent = nltk.pos_tag(nltk.word_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  three/CD\n",
      "  ex-field/JJ\n",
      "  staffers/NNS\n",
      "  ,/,\n",
      "  (PERSON Alexis/NNP Sklair/NNP)\n",
      "  ,/,\n",
      "  (PERSON Nathaniel/NNP Brown/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (PERSON Sterling/NNP Rettke/NNP)\n",
      "  ,/,\n",
      "  filed/VBD\n",
      "  suit/NN\n",
      "  in/IN\n",
      "  federal/JJ\n",
      "  court/NN\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP City/NNP)\n",
      "  ,/,\n",
      "  contending/VBG\n",
      "  that/IN\n",
      "  field/NN\n",
      "  organizers/NNS\n",
      "  were/VBD\n",
      "  fraudulently/RB\n",
      "  induced/VBN\n",
      "  to/TO\n",
      "  accept/VB\n",
      "  jobs/NNS\n",
      "  with/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Bloomberg/NNP)\n",
      "  campaign/NN\n",
      "  based/VBN\n",
      "  on/IN\n",
      "  the/DT\n",
      "  promise/NN\n",
      "  of/IN\n",
      "  guaranteed/JJ\n",
      "  salaries/NNS\n",
      "  through/IN\n",
      "  November/NNP\n",
      "  30/CD\n",
      "  ,/,\n",
      "  2020/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Get named entities from NLTK\n",
    "res = nltk.ne_chunk(sent)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from code at http://www.nltk.org/book/ch07.html to return data rather than just output it\n",
    "# by Sal Barbosa\n",
    "# This function recursively traverses a tree, collecting information into a list reference\n",
    "# Input: A tree, t, a list, and a sublist (defaults to the empty list if one is not supplied)\n",
    "# Output: Modifies the original list passed in as a reference (lst)\n",
    "def traverse_ne(t,lst,sublst=[]):\n",
    "    try:\n",
    "        t.label()\n",
    "    except AttributeError:\n",
    "        #print(t,type(t), end=\" \")\n",
    "        lst.append(t)\n",
    "    else:\n",
    "        # Now we know that t.node is defined\n",
    "        x = len(lst)\n",
    "        if t.label() != \"S\":sublst = [t.label()]\n",
    "        lst.append(sublst)      \n",
    "        for child in t:\n",
    "            traverse_ne(child,lst[x],lst[x])\n",
    "        lst[x] = tuple(lst[x])\n",
    "        #if t.label() != \"S\": sublst.insert(0, t.label())\n",
    "        #print('(', t.label(), end=\" \")\n",
    "        #print(')', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('The', 'DT'), ('three', 'CD'), ('ex-field', 'JJ'), ('staffers', 'NNS'), (',', ','), ('PERSON', ('Alexis', 'NNP'), ('Sklair', 'NNP')), (',', ','), ('PERSON', ('Nathaniel', 'NNP'), ('Brown', 'NNP')), (',', ','), ('and', 'CC'), ('PERSON', ('Sterling', 'NNP'), ('Rettke', 'NNP')), (',', ','), ('filed', 'VBD'), ('suit', 'NN'), ('in', 'IN'), ('federal', 'JJ'), ('court', 'NN'), ('in', 'IN'), ('GPE', ('New', 'NNP'), ('York', 'NNP'), ('City', 'NNP')), (',', ','), ('contending', 'VBG'), ('that', 'IN'), ('field', 'NN'), ('organizers', 'NNS'), ('were', 'VBD'), ('fraudulently', 'RB'), ('induced', 'VBN'), ('to', 'TO'), ('accept', 'VB'), ('jobs', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('ORGANIZATION', ('Bloomberg', 'NNP')), ('campaign', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('the', 'DT'), ('promise', 'NN'), ('of', 'IN'), ('guaranteed', 'JJ'), ('salaries', 'NNS'), ('through', 'IN'), ('November', 'NNP'), ('30', 'CD'), (',', ','), ('2020', 'CD'), ('.', '.'))\n"
     ]
    }
   ],
   "source": [
    "mylst = []\n",
    "traverse_ne(res,mylst)\n",
    "nes = mylst[0]\n",
    "print(nes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Alexis', 'NNP'), ('Sklair', 'NNP')) is a PERSON\n",
      "(('Nathaniel', 'NNP'), ('Brown', 'NNP')) is a PERSON\n",
      "(('Sterling', 'NNP'), ('Rettke', 'NNP')) is a PERSON\n",
      "(('New', 'NNP'), ('York', 'NNP'), ('City', 'NNP')) is a GPE\n",
      "(('Bloomberg', 'NNP'),) is a ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "# output only named entities\n",
    "for itm in nes:\n",
    "    #print(itm)\n",
    "    if isinstance(itm[1],tuple):\n",
    "        print(itm[1:],\"is a\",itm[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Relation Extraction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "# Adapted from code at http://www.nltk.org/book/ch07.html\n",
    "# using regular expressions to extract relationships between named entity types \n",
    "pat = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "#pat = re.compile(r'.*\\bof\\b')\n",
    "#pat = re.compile(r'.*\\band\\b')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = pat):\n",
    "    #for rel in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='ieer', pattern = pat):\n",
    "    #for rel in nltk.sem.extract_rels('PER', 'PER', doc, corpus='ieer', pattern = pat):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tagging (Named Entity and Part-of-Speech tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sal Barbosa\n",
    "# This function custom tags a sentence, using named entity types where applicable, and part-of-speech tags otherwise\n",
    "# Purpose: It may be used to construct chunk grammars that identify phrases based on named entities (vs. POS tags)\n",
    "# Input: It accepts a list of dictionaries output from the CoreNLP annotator with named entity tags as a property\n",
    "#        and has an optional parameter indicating whether the token, or its lemma should be returned (using lemmas defaults to False )\n",
    "# Output: It returns a list of tuples containing the token (or its lemma) and the named entity or part-of-speech tag\n",
    "def blend_pos_nes(d,uselemma=False):\n",
    "    toks = []                                           # will hold a list of tuples (token or lemma, ne or pos tag)\n",
    "    fullne = \"\"                                         # holds combined compound named entities, connecte with underscore (i.e. \"New_York\")\n",
    "    lastne = \"\"                                         # holds last named entity tage read - used to combine compound named entities\n",
    "    for t in d:                                         # t is a dictionary for each toekn in the sentence\n",
    "        if uselemma: tok = t['lemma']\n",
    "        else: tok = t['originalText']\n",
    "        pos = t['pos']\n",
    "        ne = t['ner']\n",
    "        if ne == 'O':                                   # this token is not (part of) a named entity\n",
    "            if lastne != \"\":                            # if previously \"inside\" a compound named entity (like 'New York'), terminate it\n",
    "                toks.append((fullne.strip('_'),lastne)) # and add the token and its ne tag to the list\n",
    "                lastne = \"\"                             # reset lastne and fullne\n",
    "                fullne = \"\"                             # to empty strings\n",
    "            toks.append((tok,pos))                  # not previously inside ne, so add token/lemma and pos tag\n",
    "        else:                                           # this token is (part of) a named entity\n",
    "            if ne == lastne:                            # if inside a compound named entity of the same type\n",
    "                fullne += '_' + tok                     # keep building it\n",
    "            else:                                       # otherwise this is a new named entity \n",
    "                if lastne != \"\":                            # if previously \"inside\" a different named entity than this one\n",
    "                    toks.append((fullne.strip('_'),lastne)) # close it out and add the token and its ne tag to the list              \n",
    "                lastne = ne                             # set lastne to the ne of this token\n",
    "                fullne = tok                            # begin (a possibly compound) token with this token/lemma\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sal Barbosa\n",
    "# This is a helper function that runs the NE annotator, unpacks the json return, invokes blend_pos_ne, and returns the custom tagging\n",
    "def run_ne(s):\n",
    "    props = {'annotators': 'ner','outputFormat':'json'} # set annotator to provide named entities and return as json (otherwise it's a string)\n",
    "    nes2 = nlp.annotate(s, properties=props)   # apply the annotator: results are in json format\n",
    "    d = json.loads(nes2)['sentences'][0]['tokens']\n",
    "    #print(d)\n",
    "    tagged = blend_pos_nes(d)\n",
    "    return tagged    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three ex-field staffers, Alexis Sklair, Nathaniel Brown, and Sterling Rettke, filed suit in federal court in New York City,      contending that field organizers were fraudulently induced to accept jobs with the Bloomberg campaign based on the promise of      guaranteed salaries through November 30, 2020.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "annotate() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aecd7bb604d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# set annotator to provide named entities and return as json (otherwise it's a string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# apply the annotator: results are in json format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblend_pos_nes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: annotate() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "# custom tagging output\n",
    "print(s)\n",
    "props = {'annotators': 'ner','outputFormat':'json'} # set annotator to provide named entities and return as json (otherwise it's a string)\n",
    "nes2 = nlp.annotate(s, properties=props)   # apply the annotator: results are in json format\n",
    "d = json.loads(nes2)['sentences'][0]['tokens']\n",
    "tagged = blend_pos_nes(d)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "annotate() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dd55db48cd83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4354c87d2b9d>\u001b[0m in \u001b[0;36mrun_ne\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_ne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# set annotator to provide named entities and return as json (otherwise it's a string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# apply the annotator: results are in json format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: annotate() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "tagged = run_ne(s)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom tags in a chunk grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk grammar for basic noun phrases and verb phrases\n",
    "# the second line in the grammar accepts persons in text like \"The eccentric Albert Einstein\" or simply \"George Washington\" as noun phrases\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT>?<ORDINAL>?<JJ>*<NN|NNS>+}\n",
    "      {<DT>?<JJ>*<PERSON>}\n",
    "  PP: {<IN|TO><NP>} \n",
    "  V:  {<VB|VBD|VBG|VBN|VBP|VBZ|>}\n",
    "  VP: {<V><NP><PP>*<NP>*}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "annotate() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-67e01f01fab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'The eccentric Albert Einstein once said that imagination is more important than knowledge.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(nltk.pos_tag(nltk.word_tokenize(s1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_ne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4354c87d2b9d>\u001b[0m in \u001b[0;36mrun_ne\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_ne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# set annotator to provide named entities and return as json (otherwise it's a string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# apply the annotator: results are in json format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: annotate() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "# Example of use of grammar's output with custom tagging \n",
    "s1 = 'The eccentric Albert Einstein once said that imagination is more important than knowledge.'\n",
    "#print(nltk.pos_tag(nltk.word_tokenize(s1)))\n",
    "tagged = run_ne(s1)\n",
    "print(tagged)\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(tagged))\n",
    "#s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "annotate() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-007b84f00b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The second horse ran fast.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# apply the annotator: results are in json format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblend_pos_nes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: annotate() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "line = \"The second horse ran fast.\"\n",
    "nes2 = nlp.annotate(line, properties=props)   # apply the annotator: results are in json format\n",
    "d = json.loads(nes2)['sentences'][0]['tokens']\n",
    "tagged = blend_pos_nes(d)\n",
    "print(cp.parse(tagged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
