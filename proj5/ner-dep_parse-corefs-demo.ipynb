{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Sal Barbosa\n",
    "# Purpose: Demo for 6350 on Named Entity Recognition, Dependency Parsing, Co-Reference Resolution\n",
    "import json\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "# Get an instance of StanfordCoreNLP by connecting to the server\n",
    "nlp = StanfordCoreNLP('http://jupyterlab-nfs-corenlp', port=9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Categories and Types\n",
    "# Named: ORGANIZATION, PERSON, LOCATION, MISC \n",
    "# Temporal: DATE, TIME, DURATION, SET\n",
    "# Numeric: MONEY, NUMBER, PERCENT, ORDINAL\n",
    "snt = 'Microsoft said Tuesday the plane carrying its president, Joe Schmoe, and his two children, was denied permission for the third time \\\n",
    "to land at 5 PM on December 2, 2019, for two hours, for refueling in France, where it expected to be charged $50,000 for days or weeks and assessed a 10% tax'\n",
    "snt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entities output using the annotator\n",
    "props = {'annotators': 'ner','outputFormat':'json'} # set annotator to provide lemma and get return as json (otherwise it's a string)\n",
    "res = nlp.annotate(snt, properties=props)   # apply the annotator: results are in json format\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entities output using the built-in function\n",
    "nes = nlp.ner(snt)\n",
    "nes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = '''Edward Snowden's hopes of finding asylum from U.S. prosecution on espionage charges appeared to dim Tuesday as country after country denied his request or said he would have to find a way to travel to their territory to apply. \n",
    "\n",
    "While Bolivia and Venezuela seemed supportive, 11 of the 21 countries he's applied to, including Ecuador and Iceland, have said they can't consider his request until he shows up at one of their embassies or on their borders. Three -- Brazil, India and Poland -- have denied the request outright. \n",
    "\n",
    "And Bolivia said Tuesday the plane carrying its president, Evo Morales, was denied permission to land for refueling in either France or Portugal because of \"unfounded\" rumors that Snowden was aboard. Foreign Minister David Choquehuanca told Bolivian television that the jet made an emergency landing in the Austrian capital of Vienna and that Bolivia wanted an explanation from Paris and Lisbon. \n",
    "\n",
    "\"We don't know who has come up with this huge lie,\" Choquehuanca said, adding, \"We would like to let the international community know that the rights of aerial traffic for Bolivia have been violated.\" \n",
    "\n",
    "Morales had been in Russia, where he told the Russia Today news network that he would be willing to consider asylum for Snowden. And Venezuelan President Nicolas Maduro, also in Moscow for a tribute to his late predecessor, Hugo Chavez, said Snowden deserves protection, not prosecution. \n",
    "\n",
    "Maduro said Snowden's decision to leak details of American surveillance programs were \"a warning signal to the world,\" according to statement from the president's office.'''\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function segments a multi-sentence passage into a list of strings\n",
    "# Input: a multi-sentence text passage (as a string)\n",
    "# Process: assemble the output of the sentence segmenter into a list of strings (one per sentence)\n",
    "# Output: Returns the list of individual strings (one per sentence)\n",
    "def sent_tokenize(story):\n",
    "  props={'annotators': 'ssplit','outputFormat':'json'}\n",
    "  res=nlp.annotate(story, properties=props)   # results from the annotator are in a string\n",
    "  d = json.loads(res)                         # convert it to dictionary\n",
    "  snt_lst = []                                # list of sentence strings to be returned\n",
    "  for i in range(len(d['sentences'])):\n",
    "    sd = d['sentences'][i]                    # dictionary holding information for a single sentence\n",
    "    s = \"\"                                    # sentence string to be build from dictionary info\n",
    "    for j in range(len(sd['tokens'])):        # loop through each token/word\n",
    "      wd = sd['tokens'][j]                    # dictionary holding info about a word and what precedes/follows it\n",
    "      s += wd['before'] + wd['originalText']  # append the word to the sentence string being built\n",
    "    s += wd['after']\n",
    "    snt_lst.append(s.strip())                 # append the stripped, assembled sentence string to the list\n",
    "  return snt_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sent_tokenize(story)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence strings\n",
    "toksents = []\n",
    "for i, snt in enumerate(sents):\n",
    "    toksents.append([x[0] for x in nlp.pos_tag(snt)])\n",
    "    print(i,snt)\n",
    "#toksents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt = sents[7]\n",
    "dp = nlp.dependency_parse(snt)  # returns a list of three-tuples (dependency type, head, dependent) using ONE-BASED indexing\n",
    "print(toksents[7],'\\t',len(toksents[7]), \"tokens\")\n",
    "dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-Reference Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props={'annotators': 'coref','outputFormat':'json'}\n",
    "res=nlp.annotate(story, properties=props)   # these results from the annotator are in a string\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json.loads(res)     # convert the result string into a dictionary\n",
    "crefs = d['corefs']     # extract the coreferences dictionary  \n",
    "crefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts some components from the returned coref resolution results\n",
    "# Input: story = a passage of text possibly having multiple sentences\n",
    "# Process: It transforms sentence and token indices from 1-based to 0-based, and extracts a subset of features\n",
    "# Output: It returns a dictionary containing lists of coreference chains\n",
    "def proc_corefs(story):\n",
    "  props={'annotators': 'coref','outputFormat':'json'}\n",
    "  res=nlp.annotate(story, properties=props)   # these results from the annotator are in a string\n",
    "  d = json.loads(res)                         # convert to dictionary\n",
    "  corefs = d['corefs']\n",
    "  pd = {}\n",
    "  refents = len(corefs)\n",
    "  numrefs = 0\n",
    "  for referent_k in corefs:       # cref is an entity (to which there may be multiple references)\n",
    "    pd[referent_k] = []\n",
    "    cref = corefs[referent_k]\n",
    "    numrefs += len(cref)\n",
    "    for ref in cref:        # ref is a reference to the cref being processed\n",
    "      r = {}\n",
    "      snt = ref['sentNum']-1\n",
    "      start = ref['startIndex'] -1\n",
    "      end = ref['endIndex'] - 1\n",
    "      txt = ref['text']\n",
    "\n",
    "      r['sentNum'] = snt\n",
    "      r['startIndex'] = start\n",
    "      r['endIndex'] = end\n",
    "      r['number'] = ref['number']\n",
    "      r['gender'] = ref['gender']\n",
    "      r['animacy'] = ref['animacy']\n",
    "      r['repmention'] = ref['isRepresentativeMention']\n",
    "      pd[referent_k].append(r)\n",
    "  return(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = proc_corefs(story)\n",
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in range(len(toksents)):\n",
    "    print(ts, toksents[ts])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output coreference chains/clusters\n",
    "for refid in cr:\n",
    "    print(\"Ref ID\", refid)\n",
    "    for ref in cr[refid]:\n",
    "        print(\"\\t(sent\"+str(ref['sentNum'])+\", tok\"+str(ref['startIndex'])+\")\", end=\"\")\n",
    "        for t in range(ref['startIndex'], ref['endIndex']):\n",
    "            print(\" \",toksents[ref['sentNum']][t],end=\"\")\n",
    "        if ref['repmention']: print('**')\n",
    "        else: print()\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
