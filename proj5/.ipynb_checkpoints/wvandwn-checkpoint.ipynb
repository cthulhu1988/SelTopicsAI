{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the pruned word2vec sample in NLTK\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time()\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False) # load in word2vec format\n",
    "#print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's vocabulary\n",
    "vocab = set(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: words may have to be converted between lower and title case (and possibly upper case?) to see if they exist in the vocabulary\n",
    "len(model['avocado']) # dimensions of vector representing word 'avocado' (and all other words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('wagon', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most similar using cosine similarity\n",
    "model.most_similar_cosmul('wagon', topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select most \"unlike\" item\n",
    "model.doesnt_match(['guitar', 'trumpet', 'violin', 'flute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalization and pluralization can lead to different most_similar results\n",
    "for w in ('room', 'rooms', 'Room', 'Rooms'):\n",
    "    print(w,'\\t\\n', model.most_similar(w),sep=\"\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_similar takes collections of vectors to be added (positive) or subtracted (negative) \n",
    "# Can use to specify an analogy: Read as \"Paris is to France as Madrid is to ?\"\"\n",
    "model.most_similar(positive=['France', 'Madrid'], negative=['Paris'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between two words\n",
    "model.similarity('bolt','bread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between two sets of words\n",
    "model.n_similarity(['Elena','bought','the','hat','today'], ['He','rode','his','red','wagon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similar_by_word('roadster',topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similar_by_vector(model['roadster'],topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a custom Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How-to train custom Word2Vec model (this is trained with the Brown corpus)\n",
    "custommodel = Word2Vec(brown.sents(), size=300, window=5, min_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To supress deprecation error in custom models or those loaded from saved files, prefix method call with \"wv.\"\"\n",
    "# so to call the most_similar method, use\n",
    "custommodel.wv.most_similar('wagon',topn=15)  # Note that the answers are much different from those of the pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting lemmas with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the WordNet Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lemmatizer should be given a POS tag to return the lemma (defaults to nouns otherwise, and only works for simple cases like plurals)\n",
    "# adjective = 'a'; adverb = 'r'\n",
    "print(lemmatizer.lemmatize(\"sleeping\",pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting lemmas with StanfordCoreNLP (note: this will also return the part of speech tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an instance of StanfordCoreNLP by connecting to the server\n",
    "nlp = StanfordCoreNLP('http://jupyterlab-nfs-corenlp', port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"The day was sunny and warm.  I decided I'd go boating.\"  # text to be annotated\n",
    "props = {'annotators': 'lemma','outputFormat':'json'} # set annotator to provide lemma and get return as json (otherwise it's a string)\n",
    "res = nlp.annotate(txt, properties=props)   # apply the annotator: results are in json format\n",
    "d = json.loads(res)                         # load the json object into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d is the dictionary returned from loading the json response\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d['sentences'] is a list of dictionaries, one per sentence - so the second sentence (at index 1) is\n",
    "first_sent_d = d['sentences'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each dictionary has a key 'index' (value is integer sentence index) and a key 'tokens' (value is list of token dictionaries)\n",
    "first_sent_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each token dictionary contains (among other things) the original text, its corresponding lemma, and the POS tag:\n",
    "for tok_d in first_sent_d['tokens']:\n",
    "    print(tok_d['originalText'], tok_d['lemma'], tok_d['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import WordNet\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('dog')  # Returns the synsets for the word \"dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('dog', pos='n')  # Restrict returned synsets to verbs (also works with NOUN, ADJ, ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a specific synset\n",
    "dog = wn.synset('dog.n.01')\n",
    "dog.name() # dentifies the name of the synset of this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.lemmas() # Outputs full lemmas, including part of speech and sense #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.lemma_names() # Outputs lemma names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.definition() # The gloss (or definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.examples() # examples of use(s) in sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syns_ants(word):\n",
    "    synonyms = [] \n",
    "    antonyms = []      \n",
    "    for syn in wn.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonyms.append(l.name()) \n",
    "            if l.antonyms(): \n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "    synonyms = set(synonyms)\n",
    "    antonyms = set(antonyms)\n",
    "    return synonyms, antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns, ants = syns_ants(\"calm\")\n",
    "\n",
    "print(\"Synonyms:\",syns,\"\\n\\nAntonyms:\",ants,sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypernyms and Hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.hypernyms() # Hypernyms for dog (e.g., a dog is-a ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.hyponyms() # Hyponyms for dog (e.g., each of these is-a dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.hypernym_paths() # Paths to all hypernyms (as returned by hypernyms method above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypernym Tree\n",
    "from pprint import pprint  # import pretty print \n",
    "hyp = lambda s:s.hypernyms() \n",
    "pprint(dog.tree(hyp)) # output hypernym tree \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = wn.synset('cat.n.01') # get cat synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.common_hypernyms(cat) # what hypernyms are common to both dogs and cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest Common Hypernym between two synsets\n",
    "wn.synset('hairdresser.n.01').lowest_common_hypernyms(wn.synset('chef.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Hypernyms/Hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=wn.synset('city.n.01') \n",
    "city.instance_hyponyms()[:10] # Lists the first 10 hyponyms of city (individual cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aca = wn.synset('acapulco.n.01')  # pick a city\n",
    "aca.instance_hypernyms() # Lists all its hypernyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car=wn.synset('car.n.01') # get synset for car\n",
    "car.part_meronyms()[:10]  # Returns top 10 entities that compose a car (accelerator, airbag, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accel = wn.synset('accelerator.n.01')  # get the synset for accelerator\n",
    "accel.part_holonyms()                  # Returns holonyms for the synset: entities that the accelerator is a part-of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Member Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = wn.synset('forest.n.01')  # get synset for forest\n",
    "forest.member_meronyms()           # Returns entities that are member-of a forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=wn.synset('tree.n.01')  # get the tree synset\n",
    "tree.member_holonyms()       # Returns entities that the tree is a member-of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substance Holonyms and Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread= wn.synset('bread.n.01')  # get synset for bread\n",
    "bread.substance_meronyms()      # Returns entities that are substances of bread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flour = wn.synset('flour.n.01')  # get synset for flour\n",
    "flour.substance_holonyms()       # Returns entities flour is a substances-of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path-Based Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "love = wn.synset('love.n.01')\n",
    "romance = wn.synset('romance.n.01')\n",
    "hate = wn.synset('hate.n.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Path Similarity** (returns 0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity is not synonymy\n",
    "print('Love - Romance',love.path_similarity(romance))\n",
    "print('Love - Hate',love.path_similarity(hate))\n",
    "print('Romance - Hate',romance.path_similarity(hate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leacock-Chodorow Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Love - Romance',love.lch_similarity(romance))\n",
    "print('Love - Hate',love.lch_similarity(hate))\n",
    "print('Romance - Hate',romance.lch_similarity(hate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wu-Palmer Similarity** (returns 0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Love - Romance',love.wup_similarity(romance))\n",
    "print('Love - Hate',love.wup_similarity(hate))\n",
    "print('Romance - Hate',romance.wup_similarity(hate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information-Content Based Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic as wic  # import that allows loading of information content\n",
    "ic = wic.ic('ic-brown.dat')                # load information content from the Brown corpus int variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lin Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Love - Romance',love.lin_similarity(romance, ic))\n",
    "print('Love - Hate',love.lin_similarity(hate, ic))\n",
    "print('Romance - Hate',romance.lin_similarity(hate, ic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resnik Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Love - Romance',love.res_similarity(romance, ic))\n",
    "print('Love - Hate',love.res_similarity(hate, ic))\n",
    "print('Romance - Hate',romance.res_similarity(hate, ic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jiang-Conrath Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Love - Romance',love.jcn_similarity(romance, ic))\n",
    "print('Love - Hate',love.jcn_similarity(hate, ic))\n",
    "print('Romance - Hate',romance.jcn_similarity(hate, ic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
