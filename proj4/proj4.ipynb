{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <blockquote>\n",
    " Isaac Callison<br>\n",
    " CSCI 6350-001<br>\n",
    " Project 4<br>\n",
    " Due: 02/22/2020<br>\n",
    " </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the things. \n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import opinion_lexicon as op\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "np.set_printoptions(linewidth=400)   # optional: widens column of numpy array display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get positve and negative words from the lexicon. \n",
    "pos_words = set(op.words('positive-words.txt'))\n",
    "neg_words = set(op.words('negative-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create initial list to work on \n",
    "X = [] ; y = []; initial_set = []; X_test = []; y_test = []\n",
    "with open(\"reviews.txt\", 'r', encoding='utf8') as reviews:\n",
    "    for review in reviews:\n",
    "        if review != \"\":\n",
    "            review = review.strip()\n",
    "            review = review.lower()\n",
    "            #review = word_tokenize(review)\n",
    "            if len(review) > 1:\n",
    "                initial_set.append(review)\n",
    "num_sentences = len(initial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Since the data predominated with 5's I developed a cumbersome method to makes sure the data trained on was \n",
    "############# representative of the different classes. I made sure the poorest represented class was fully represented.\n",
    "my_dict = defaultdict(list)\n",
    "\n",
    "### Append each review to a rating key\n",
    "for item in (initial_set):\n",
    "    my_dict[int(item[-1])].append(item)\n",
    "\n",
    "### Find the shortest list of reviews, in this case it is 1    \n",
    "leng = 1000000\n",
    "for key,value in my_dict.items():\n",
    "    v = -1\n",
    "    if len(value) < leng:\n",
    "        leng = len(value)\n",
    "        v = key\n",
    "\n",
    "#### write out each rating to the training file making sure each is represented        \n",
    "count = 0\n",
    "with open(\"reviews-train.txt\", 'w') as r_train:\n",
    "    for idx in range(0,leng):\n",
    "        for key,value in my_dict.items():\n",
    "             r_train.write(value[idx])\n",
    "             r_train.write('\\n')   \n",
    "             count+=1\n",
    "\n",
    "#### write the rest out to a file.                 \n",
    "test_set = initial_set[count:]\n",
    "with open(\"reviews-test.txt\", 'w') as r_test:\n",
    "    for line in test_set:\n",
    "        r_test.write(line)\n",
    "        r_test.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## this was my attempt to develop a rating system based on number of positives versus number of negatives.\n",
    "\n",
    "def return_rating(posCount, negCount):\n",
    "        stars = 0\n",
    "        if negCount > (posCount*2):\n",
    "            stars = 1\n",
    "        elif posCount > (negCount*2):\n",
    "            stars = 5\n",
    "        elif negCount == posCount:\n",
    "            stars = 3\n",
    "        elif negCount < posCount:\n",
    "            stars = 4\n",
    "        elif posCount < negCount:\n",
    "            stars = 2\n",
    "        return stars\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## open each file, read into memory, strip excess white space, and break into target and features. \n",
    "rev_train_file = open('reviews-train.txt')\n",
    "lines = rev_train_file.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    X.append(word_tokenize(line[:-1]))\n",
    "    y.append(line[-1])\n",
    " \n",
    "rev_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## open each file, read into memory, strip excess white space, and break into target and features. \n",
    "rev_test_file = open('reviews-test.txt')\n",
    "lines = rev_test_file.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    X_test.append(word_tokenize(line[:-1]))\n",
    "    y_test.append(line[-1])\n",
    "rev_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function just returned the number of positive and negative words from the opinion lexicon\n",
    "\n",
    "def pos_neg_count(line):\n",
    "    posCount = 0; negCount = 0\n",
    "    for word in line:\n",
    "        if word in pos_words:\n",
    "            posCount +=1\n",
    "        elif word in neg_words:\n",
    "            negCount+=1\n",
    "    return posCount, negCount\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## These are the most common tags across the review set ######\n",
    "def superlative_count(tokenized, pos_d):\n",
    "    pos_dict = pos_d\n",
    "    for i in tokenized[:1]:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "    ######## Array is incremented if one of these parts of speech shows up. \n",
    "     ########0,1,2,3,4,5,6,7,8,9,10\n",
    "    count = [0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for x in tagged:\n",
    "        POS = x[1]\n",
    "        pos_dict[POS] = pos_dict.get(POS,0) +1\n",
    "        if(POS) == 'RB':\n",
    "            count[0] +=1\n",
    "        if(POS) == 'NN':\n",
    "            count[1] +=1\n",
    "        if(POS) == 'VBP':\n",
    "            count[2] +=1\n",
    "        if(POS) == 'JJS':\n",
    "            count[3] +=1\n",
    "        if(POS) == 'IN':\n",
    "            count[4] +=1\n",
    "        if(POS) == 'DT':\n",
    "            count[5] +=1\n",
    "        if(POS) == 'NNS':\n",
    "            count[6] +=1\n",
    "        if(POS) == 'VBZ':\n",
    "            count[7] +=1   \n",
    "        if(POS) == 'JJ':\n",
    "            count[8] +=1\n",
    "        if(POS) == 'EX':\n",
    "            count[9] +=1     \n",
    "    ######## The array of counts and the accumulating POS dictionary are returned. \n",
    "    return count, pos_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "############################################ FEATURE EXTRACTION ###################################################\n",
    "###################################################################################################################\n",
    "def np_feature_extraction(X,y, pos_dict):\n",
    "    X_list = [] ; y_list = []\n",
    "    pos_d = pos_dict\n",
    "    punktLine = PunktSentenceTokenizer()\n",
    "    for x,line in enumerate(X):\n",
    "        ### GET TARGET #####\n",
    "        rating = int(y[x])\n",
    "     \n",
    "        # GET POS AND NEG COUNTS\n",
    "        posCount, negCount = pos_neg_count(line)\n",
    "        \n",
    "        ## POS TAGGING\n",
    "        sent = ' '.join(line)\n",
    "        token = punktLine.tokenize(sent)\n",
    "        superlative,pos = superlative_count(token, pos_d)\n",
    "        \n",
    "        ### Apply guessed rating\n",
    "        rating_result = return_rating(posCount, negCount)\n",
    "        \n",
    "        ## THIS IS THE NUMPY LINE WITH FEATURES\n",
    "        temp_line = [posCount, negCount, rating_result] + superlative\n",
    "        line = temp_line\n",
    "        #print(line)\n",
    "     \n",
    "        ### APPEND LINES #####\n",
    "        X_list.append(line)\n",
    "        y_list.append(rating)\n",
    "        \n",
    "    ##TURN INTO NUMPY ARRAYS\n",
    "    X_np = (np.array(X_list))\n",
    "    y_np = (np.array(y_list))\n",
    "    \n",
    "    return X_np,y_np, pos_d\n",
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {}\n",
    "X_train_np, y_train_np, pos = np_feature_extraction(X,y, pos_dict)\n",
    "X_test_np, y_test_np, pos = np_feature_extraction(X_test, y_test, pos_dict)\n",
    "\n",
    "############ This is the total number of each of the speech tags in the document. \n",
    "#### Uncomment to see the dictionary:\n",
    "#print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " multinomial  Accuracy 0.3841707425522454\n",
      "\n",
      "multinomial Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.09      0.51      0.16        43\n",
      "           2       0.04      0.15      0.07        61\n",
      "           3       0.10      0.25      0.14       161\n",
      "           4       0.24      0.24      0.24       464\n",
      "           5       0.75      0.45      0.56      1520\n",
      "\n",
      "    accuracy                           0.38      2249\n",
      "   macro avg       0.24      0.32      0.23      2249\n",
      "weighted avg       0.57      0.38      0.44      2249\n",
      "\n",
      "\n",
      "multinomial Confusion Matrix\n",
      "[[ 22   4   8   4   5]\n",
      " [ 17   9  13  10  12]\n",
      " [ 36  17  41  29  38]\n",
      " [ 46  54  84 113 167]\n",
      " [111 130 278 322 679]]\n",
      "\n",
      " ovr  Accuracy 0.3863939528679413\n",
      "\n",
      "ovr Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.09      0.56      0.15        43\n",
      "           2       0.03      0.08      0.05        61\n",
      "           3       0.10      0.24      0.14       161\n",
      "           4       0.23      0.25      0.24       464\n",
      "           5       0.75      0.45      0.56      1520\n",
      "\n",
      "    accuracy                           0.39      2249\n",
      "   macro avg       0.24      0.32      0.23      2249\n",
      "weighted avg       0.56      0.39      0.44      2249\n",
      "\n",
      "\n",
      "ovr Confusion Matrix\n",
      "[[ 24   2   5   6   6]\n",
      " [ 20   5  10  12  14]\n",
      " [ 38  14  38  33  38]\n",
      " [ 59  36  78 116 175]\n",
      " [129  96 268 341 686]]\n"
     ]
    }
   ],
   "source": [
    "for mclass in ('multinomial', 'ovr'):\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=3000, random_state=0, multi_class=mclass).fit(X_train_np, y_train_np)\n",
    "    yhat = lr.predict(X_test_np)\n",
    "     \n",
    "    # the 3 lines below show how to invoke various output    \n",
    "    print(\"\\n\",mclass,\" Accuracy\",accuracy_score(y_test_np, yhat))\n",
    "    print(\"\\n\",mclass,\" Classification Report\\n\",classification_report(y_test_np, yhat),sep=\"\")\n",
    "    print(\"\\n\",mclass,\" Confusion Matrix\\n\",confusion_matrix(y_test_np, yhat),sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "Selection of features â€“ Explain how you arrived at your final set of features: what did you try? What insight lead you to them? Were there one or more that were particularly good?\n",
    "\n",
    "ANSWER: I tried length of review, but that gave me worse results. I tried one hot encoding, but that did not seem to give any better results than just a basic rating system based on positive and negative words from the NLTK lexicon. I included the postive and negative words from the lexicon as raw numbers. I tried POS tagging by applying points based on the type of superlative. Then I created a dictionary to count the number of times each part of speech actually showed up, counted them as features, and inserted them into my feature list. \n",
    "    \n",
    "With regard to the training and testing data, I found that the program could not discern any other categories except for 5 star ratings, and that this was probably due to the fact that 5 star ratings predominated in the reviews.txt file. Unfortunately I realized this late, and my implementation of a balanced training set resulted in a 10/90 split of training/testing data.   \n",
    "\n",
    "Provide your thoughts on the differences observed between one versus rest and multinomial classification. \n",
    "\n",
    "ANSWER: From my scores, there was not a whole lot of difference. I know the OVR method looks at one and differentiates the rest, and the MN looks at differences among all of them. My results were so low I am not sure any true differences came out. \n",
    "\n",
    "Provide an analysis of your measures (accuracy, f1, precision, recall) and the confusion matrix. \n",
    "\n",
    "ANSWER: My accuracy was very poor regardless of using multinomial or OVR. The multinomial method did show slightly higher results over the OVR. Recall was highest among the 1 start ratings, but I included every 1 star rating available across the set. Precision and F1 was the highest for the 5 star rating, no surprise as the bulk of the reviews were in fact 5 stars. \n",
    "    \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
