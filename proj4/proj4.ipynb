{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \n",
    " Isaac Callison\n",
    " CSCI 6350-001\n",
    " Project 4\n",
    " Due: 02/18/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the things. \n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import opinion_lexicon as op\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "np.set_printoptions(linewidth=400)   # optional: widens column of numpy array display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get positve and negative words from the lexicon. \n",
    "pos_words = set(op.words('positive-words.txt'))\n",
    "neg_words = set(op.words('negative-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create lists for response and target. \n",
    "X = [] ; y = []\n",
    "with open(\"reviews.txt\", 'r', encoding='utf8') as reviews:\n",
    "    for review in reviews:\n",
    "        if review != \"\":\n",
    "            review = review.strip()\n",
    "            review = review.lower()\n",
    "            review = word_tokenize(review)\n",
    "            X.append(review[:-1])\n",
    "            y.append(int(review[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open CSV file and record data line by line to read later. \n",
    "\n",
    "with open(\"reviews.csv\", 'w', newline='') as rv:\n",
    "    thefile = csv.writer(rv)\n",
    "    thefile.writerow(['leng', 'rating'])\n",
    "    for x,line in enumerate(X):\n",
    "        posCount = 0 ; negCount = 0\n",
    "        # create features of positive and negative words. \n",
    "        leng = len(line)\n",
    "        for word in line:\n",
    "            if word in pos_words:\n",
    "                posCount +=1\n",
    "            elif word in neg_words:\n",
    "                negCount+=1\n",
    "        # add a positive rating based on how much more pos reviews show over neg.\n",
    "        one_star = 0; two_star = 0; three_star = 0; four_star=0; five_star = 0\n",
    "        if negCount > (posCount*2):\n",
    "            one_star = 1\n",
    "        elif posCount > (negCount*2):\n",
    "            five_star = 1\n",
    "        elif negCount == posCount:\n",
    "            three_star = 1\n",
    "        elif negCount < posCount:\n",
    "            four_star= 1\n",
    "        elif posCount < negCount:\n",
    "            two_star = 1\n",
    "            \n",
    "\n",
    "        #This is the tag\n",
    "        rating = y[x]\n",
    "        test_rating = y[x]\n",
    "        line = [leng,one_star, two_star, three_star, four_star, five_star, rating]\n",
    "        # write data to CSV file for later reading. \n",
    "        #print(line)\n",
    "        thefile.writerow(line)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"reviews.csv\", header=0)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "y = df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " multinomial Accuracy 0.688\n",
      "\n",
      "multinomialClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        42\n",
      "           4       0.00      0.00      0.00       102\n",
      "           5       0.69      1.00      0.82       344\n",
      "\n",
      "    accuracy                           0.69       500\n",
      "   macro avg       0.14      0.20      0.16       500\n",
      "weighted avg       0.47      0.69      0.56       500\n",
      "\n",
      "\n",
      "multinomialClassification Report\n",
      "[[  0   0   0   0   2]\n",
      " [  0   0   0   0  10]\n",
      " [  0   0   0   0  42]\n",
      " [  0   0   0   0 102]\n",
      " [  0   0   0   0 344]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loki/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ovr Accuracy 0.688\n",
      "\n",
      "ovrClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        42\n",
      "           4       0.00      0.00      0.00       102\n",
      "           5       0.69      1.00      0.82       344\n",
      "\n",
      "    accuracy                           0.69       500\n",
      "   macro avg       0.14      0.20      0.16       500\n",
      "weighted avg       0.47      0.69      0.56       500\n",
      "\n",
      "\n",
      "ovrClassification Report\n",
      "[[  0   0   0   0   2]\n",
      " [  0   0   0   0  10]\n",
      " [  0   0   0   0  42]\n",
      " [  0   0   0   0 102]\n",
      " [  0   0   0   0 344]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loki/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for mclass in ('multinomial', 'ovr'):\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=30000, random_state=0, multi_class=mclass).fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "     \n",
    "    # the 3 lines below show how to invoke various output    \n",
    "    print(\"\\n\",mclass,\"Accuracy\",accuracy_score(y_test, yhat))\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",classification_report(y_test, yhat),sep=\"\")\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",confusion_matrix(y_test, yhat),sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
