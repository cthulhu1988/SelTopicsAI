{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \n",
    " Isaac Callison\n",
    " CSCI 6350-001\n",
    " Project 4\n",
    " Due: 02/18/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the things. \n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import opinion_lexicon as op\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "np.set_printoptions(linewidth=400)   # optional: widens column of numpy array display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get positve and negative words from the lexicon. \n",
    "pos_words = set(op.words('positive-words.txt'))\n",
    "neg_words = set(op.words('negative-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create lists for response and target. \n",
    "X = [] ; y = []; initial_set = []; X_test = []; y_test = []\n",
    "with open(\"reviews.txt\", 'r', encoding='utf8') as reviews:\n",
    "    for review in reviews:\n",
    "        if review != \"\":\n",
    "            review = review.strip()\n",
    "            review = review.lower()\n",
    "            #review = word_tokenize(review)\n",
    "            if len(review) > 1:\n",
    "                initial_set.append(review)\n",
    "    random.shuffle(initial_set)\n",
    "num_sentences = len(initial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# write out a split to files:\n",
    "train_len = int(0.8 * num_sentences)\n",
    "test_len = num_sentences - train_len\n",
    "\n",
    "train_list = initial_set[:train_len]\n",
    "test_list = initial_set[train_len:]\n",
    "print(len(train_list))\n",
    "print(len(test_list))\n",
    "\n",
    "with open('reviews-train.txt', 'w') as r_train:\n",
    "    for line in train_list:\n",
    "        line = str(line)\n",
    "        r_train.write(line)\n",
    "        r_train.write('\\n')\n",
    "with open('reviews-test.txt', 'w') as r_test:\n",
    "    for test_line in test_list:\n",
    "        test_line = str(test_line)\n",
    "        r_test.write(test_line)\n",
    "        r_test.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rating(pos, neg):\n",
    "        stars = [0,0,0,0,0]\n",
    "        if negCount > (posCount*2):\n",
    "            stars[0] = 1\n",
    "        elif posCount > (negCount*2):\n",
    "            stars[4] = 1\n",
    "        elif negCount == posCount:\n",
    "            stars[2] = 1\n",
    "        elif negCount < posCount:\n",
    "            stars[3]= 1\n",
    "        elif posCount < negCount:\n",
    "            stars[1] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rev_train_file = open('reviews-train.txt')\n",
    "rev_train_file = open('reviews-train.txt')\n",
    "\n",
    "lines = rev_train_file.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    X.append(word_tokenize(line[:-1]))\n",
    "    y.append(line[-1])\n",
    "    \n",
    "rev_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rev_train_file = open('reviews-train.txt')\n",
    "rev_test_file = open('reviews-test.txt')\n",
    "\n",
    "lines = rev_test_file.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    X_test.append(word_tokenize(line[:-1]))\n",
    "    y_test.append(line[-1])\n",
    "    \n",
    "####rev_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "############################################ FEATURE EXTRACTION ###################################################\n",
    "###################################################################################################################\n",
    "def np_feature_extraction(X,y):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    my_dict = {}\n",
    "    \n",
    "    for x,line in enumerate(X):\n",
    "        rating = int(y[x])\n",
    "        posCount = 0 ; negCount = 0\n",
    "        # create features of positive and negative words. \n",
    "        leng = len(line)\n",
    "        for word in line:\n",
    "            if word in pos_words:\n",
    "                posCount +=1\n",
    "            elif word in neg_words:\n",
    "                negCount+=1\n",
    "        ## Check number of ratings. \n",
    "        my_dict[rating]=my_dict.get(rating,0)+1\n",
    "        \n",
    "        line = [leng, posCount, negCount]\n",
    "        X_list.append(line)\n",
    "        y_list.append(rating)\n",
    "    \n",
    "    print(my_dict)\n",
    "    \n",
    "    X_np = (np.array(X_list))\n",
    "    y_np = (np.array(y_list))\n",
    "    \n",
    "    return X_np,y_np\n",
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 393, 5: 1367, 3: 147, 2: 56, 1: 36}\n",
      "{3: 25, 5: 342, 4: 101, 1: 14, 2: 18}\n"
     ]
    }
   ],
   "source": [
    "X_train_np, y_train_np = np_feature_extraction(X,y)\n",
    "X_test_np, y_test_np = np_feature_extraction(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " multinomial Accuracy 1.0\n",
      "\n",
      "multinomialClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00        18\n",
      "           3       1.00      1.00      1.00        25\n",
      "           4       1.00      1.00      1.00       101\n",
      "           5       1.00      1.00      1.00       342\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       500\n",
      "   macro avg       1.00      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n",
      "\n",
      "multinomialClassification Report\n",
      "[[ 14   0   0   0   0]\n",
      " [  0  18   0   0   0]\n",
      " [  0   0  25   0   0]\n",
      " [  0   0   0 101   0]\n",
      " [  0   0   0   0 342]]\n",
      "\n",
      " ovr Accuracy 0.91\n",
      "\n",
      "ovrClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.86      0.92        14\n",
      "           2       0.00      0.00      0.00        18\n",
      "           3       0.00      0.00      0.00        25\n",
      "           4       0.71      1.00      0.83       101\n",
      "           5       1.00      1.00      1.00       342\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       500\n",
      "   macro avg       0.54      0.57      0.55       500\n",
      "weighted avg       0.85      0.91      0.88       500\n",
      "\n",
      "\n",
      "ovrClassification Report\n",
      "[[ 12   0   2   0   0]\n",
      " [  0   0   1  17   0]\n",
      " [  0   0   0  25   0]\n",
      " [  0   0   0 101   0]\n",
      " [  0   0   0   0 342]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loki/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/loki/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for mclass in ('multinomial', 'ovr'):\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=3000, random_state=0, multi_class=mclass).fit(X_train_np, y_train_np)\n",
    "    yhat = lr.predict(X_test_np)\n",
    "     \n",
    "    # the 3 lines below show how to invoke various output    \n",
    "    print(\"\\n\",mclass,\"Accuracy\",accuracy_score(y_test_np, yhat))\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",classification_report(y_test_np, yhat),sep=\"\")\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",confusion_matrix(y_test_np, yhat),sep=\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Selection of features â€“ Explain how you arrived at your final set of features: what did you try? What insight lead you to them? Were there one or more that were particularly good?\n",
    "ANSWER:\n",
    "\n",
    "Provide your thoughts on the differences observed between one versus rest and multinomial classification. \n",
    "ANSWER:\n",
    "\n",
    "Provide an analysis of your measures (accuracy, f1, precision, recall) and the confusion matrix. \n",
    "ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
