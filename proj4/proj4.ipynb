{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \n",
    " Isaac Callison\n",
    " CSCI 6350-001\n",
    " Project 4\n",
    " Due: 02/18/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the things. \n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import opinion_lexicon as op\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "np.set_printoptions(linewidth=400)   # optional: widens column of numpy array display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get positve and negative words from the lexicon. \n",
    "pos_words = set(op.words('positive-words.txt'))\n",
    "neg_words = set(op.words('negative-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create initial list to work on \n",
    "X = [] ; y = []; initial_set = []; X_test = []; y_test = []\n",
    "with open(\"reviews.txt\", 'r', encoding='utf8') as reviews:\n",
    "    for review in reviews:\n",
    "        if review != \"\":\n",
    "            review = review.strip()\n",
    "            review = review.lower()\n",
    "            #review = word_tokenize(review)\n",
    "            if len(review) > 1:\n",
    "                initial_set.append(review)\n",
    "num_sentences = len(initial_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Since the data predominated with 5's I developed a cumbersome method to makes sure the data trained on was \n",
    "############# representative of the different classes. I made sure the poorest represented class was fully represented.\n",
    "my_dict = defaultdict(list)\n",
    "\n",
    "### Append each review to a rating key\n",
    "for item in (initial_set):\n",
    "    my_dict[int(item[-1])].append(item)\n",
    "\n",
    "### Find the shortest list of reviews, in this case it is 1    \n",
    "leng = 1000000\n",
    "for key,value in my_dict.items():\n",
    "    v = -1\n",
    "    if len(value) < leng:\n",
    "        leng = len(value)\n",
    "        v = key\n",
    "\n",
    "#### write out each rating to the training file making sure each is represented        \n",
    "count = 0\n",
    "with open(\"reviews-train.txt\", 'w') as r_train:\n",
    "    for idx in range(0,leng):\n",
    "        for key,value in dates_dict.items():\n",
    "             r_train.write(value[idx])\n",
    "             r_train.write('\\n')   \n",
    "             count+=1\n",
    "\n",
    "#### write the rest out to a file.                 \n",
    "test_set = initial_set[count:]\n",
    "with open(\"reviews-test.txt\", 'w') as r_test:\n",
    "    for line in test_set:\n",
    "        r_test.write(line)\n",
    "        r_test.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## this was my attempt to develop a rating system based on number of positives versus number of negatives.\n",
    "\n",
    "def return_rating(posCount, negCount):\n",
    "        stars = 0\n",
    "        if negCount > (posCount*2):\n",
    "            stars = 1\n",
    "        elif posCount > (negCount*2):\n",
    "            stars = 5\n",
    "        elif negCount == posCount:\n",
    "            stars = 3\n",
    "        elif negCount < posCount:\n",
    "            stars = 4\n",
    "        elif posCount < negCount:\n",
    "            stars = 2\n",
    "        return stars\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## open each file, read into memory, strip excess white space, and break into target and features. \n",
    "rev_train_file = open('reviews-train.txt')\n",
    "lines = rev_train_file.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    X.append(word_tokenize(line[:-1]))\n",
    "    y.append(line[-1])\n",
    " \n",
    "rev_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## open each file, read into memory, strip excess white space, and break into target and features. \n",
    "rev_test_file = open('reviews-test.txt')\n",
    "lines = rev_test_file.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    X_test.append(word_tokenize(line[:-1]))\n",
    "    y_test.append(line[-1])\n",
    "rev_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function just returned the number of positive and negative words from the opinion lexicon\n",
    "\n",
    "def pos_neg_count(line):\n",
    "    posCount = 0; negCount = 0\n",
    "    for word in line:\n",
    "        if word in pos_words:\n",
    "            posCount +=1\n",
    "        elif word in neg_words:\n",
    "            negCount+=1\n",
    "    return posCount, negCount\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superlative_count(tokenized):\n",
    "    \n",
    "    for i in tokenized[:1]:\n",
    "        words = nltk.word_tokenize(i)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "    count = 0\n",
    "    for x in tagged:\n",
    "        if (x[1]) == 'JJS':\n",
    "            count +=6\n",
    "        if (x[1]) == 'JJR':\n",
    "            count +=3\n",
    "        if (x[1]) == 'JJ':\n",
    "            count +=1\n",
    "        if (x[1]) == 'RBS':\n",
    "            count += 6\n",
    "        if (x[1]) == 'PRP':\n",
    "            count += 6\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "############################################ FEATURE EXTRACTION ###################################################\n",
    "###################################################################################################################\n",
    "def np_feature_extraction(X,y):\n",
    "    X_list = [] ; y_list = []\n",
    "\n",
    "    punktLine = PunktSentenceTokenizer()\n",
    "    for x,line in enumerate(X):\n",
    "        ### GET TARGET #####\n",
    "        rating = int(y[x])\n",
    "     \n",
    "        # GET POS AND NEG COUNTS\n",
    "        posCount, negCount = pos_neg_count(line)\n",
    "        ## POS TAGGING SUPERLATIVES\n",
    "        sent = ' '.join(line)\n",
    "        token = punktLine.tokenize(sent)\n",
    "        superlative = superlative_count(token)\n",
    "        ### Apply guessed rating\n",
    "        rating_result = return_rating(posCount, negCount)\n",
    "        \n",
    "        ## THIS IS THE NUMPY LINE WITH FEATURES\n",
    "        line = [posCount, negCount, superlative, rating_result]\n",
    "     \n",
    "        ### APPEND LINES #####\n",
    "        X_list.append(line)\n",
    "        y_list.append(rating)\n",
    "        \n",
    "    ##TURN INTO NUMPY ARRAYS\n",
    "    X_np = (np.array(X_list))\n",
    "    y_np = (np.array(y_list))\n",
    "    \n",
    "    return X_np,y_np\n",
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, y_train_np = np_feature_extraction(X,y)\n",
    "X_test_np, y_test_np = np_feature_extraction(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " multinomial Accuracy 0.49444197421076036\n",
      "\n",
      "multinomialClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.09      0.53      0.15        43\n",
      "           2       0.04      0.10      0.06        61\n",
      "           3       0.10      0.19      0.13       161\n",
      "           4       0.29      0.12      0.17       464\n",
      "           5       0.74      0.66      0.69      1520\n",
      "\n",
      "    accuracy                           0.49      2249\n",
      "   macro avg       0.25      0.32      0.24      2249\n",
      "weighted avg       0.57      0.49      0.52      2249\n",
      "\n",
      "\n",
      "multinomialClassification Report\n",
      "[[ 23   3   5   5   7]\n",
      " [ 21   6   9   5  20]\n",
      " [ 34  15  30  14  68]\n",
      " [ 49  36  59  57 263]\n",
      " [135  76 200 113 996]]\n",
      "\n",
      " ovr Accuracy 0.49755446865273456\n",
      "\n",
      "ovrClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.08      0.56      0.14        43\n",
      "           2       0.00      0.00      0.00        61\n",
      "           3       0.10      0.20      0.13       161\n",
      "           4       0.29      0.16      0.21       464\n",
      "           5       0.74      0.65      0.69      1520\n",
      "\n",
      "    accuracy                           0.50      2249\n",
      "   macro avg       0.24      0.31      0.23      2249\n",
      "weighted avg       0.57      0.50      0.52      2249\n",
      "\n",
      "\n",
      "ovrClassification Report\n",
      "[[ 24   1   4   7   7]\n",
      " [ 22   0  11   7  21]\n",
      " [ 37   5  32  18  69]\n",
      " [ 57  12  63  74 258]\n",
      " [156  22 208 145 989]]\n"
     ]
    }
   ],
   "source": [
    "for mclass in ('multinomial', 'ovr'):\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=30000, random_state=0, multi_class=mclass).fit(X_train_np, y_train_np)\n",
    "    yhat = lr.predict(X_test_np)\n",
    "     \n",
    "    # the 3 lines below show how to invoke various output    \n",
    "    print(\"\\n\",mclass,\"Accuracy\",accuracy_score(y_test_np, yhat))\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",classification_report(y_test_np, yhat),sep=\"\")\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",confusion_matrix(y_test_np, yhat),sep=\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Selection of features â€“ Explain how you arrived at your final set of features: what did you try? What insight lead you to them? Were there one or more that were particularly good?\n",
    "\n",
    "ANSWER: I tried one hot encoding, but that did not seem to give any better results than just a basic rating system based on positive and negative words from the NLTK lexicon. I included the postive and negative words from the lexicon as raw numbers. I put in a little POS tagging by applying points based on the type of superlative. If I had more time I would have found some tags that indicated a negative review and deducted from this rating system.  \n",
    "\n",
    "Provide your thoughts on the differences observed between one versus rest and multinomial classification. \n",
    "ANSWER:\n",
    "\n",
    "Provide an analysis of your measures (accuracy, f1, precision, recall) and the confusion matrix. \n",
    "ANSWER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
