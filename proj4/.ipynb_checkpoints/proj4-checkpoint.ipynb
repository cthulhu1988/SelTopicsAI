{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \n",
    " Isaac Callison\n",
    " CSCI 6350-001\n",
    " Project 4\n",
    " Due: 02/18/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import opinion_lexicon as op\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "np.set_printoptions(linewidth=400)   # optional: widens column of numpy array display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set(op.words('positive-words.txt'))\n",
    "neg_words = set(op.words('negative-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "feature_cols = [\"word_count\", \"pos\"]\n",
    "\n",
    "\n",
    "with open(\"reviews.txt\", 'r', encoding='utf8') as reviews:\n",
    "    for review in reviews:\n",
    "        if review != \"\":\n",
    "            review = review.strip()\n",
    "            review = review.lower()\n",
    "            review = word_tokenize(review)\n",
    "            X.append(review[:-1])\n",
    "            y.append(int(review[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reviews.csv\", 'w', newline='') as rv:\n",
    "    thefile = csv.writer(rv)\n",
    "    thefile.writerow(['len_word', 'pos', 'neg', 'rating'])\n",
    "    for x,line in enumerate(X):\n",
    "        posCount = 0 ; negCount = 0\n",
    "        \n",
    "        leng = len(line)\n",
    "        for word in line:\n",
    "            if word in pos_words:\n",
    "                posCount +=1\n",
    "            elif word in neg_words:\n",
    "                negCount+=1\n",
    "        value = y[x]\n",
    "        line = [leng,posCount,negCount,value]\n",
    "        #print(line)\n",
    "        thefile.writerow(line)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2499, 4)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"reviews.csv\", header=0)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('rating', axis=1)\n",
    "y = df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " multinomial Accuracy 0.684\n",
      "\n",
      "multinomialClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        42\n",
      "           4       0.00      0.00      0.00       102\n",
      "           5       0.69      0.99      0.81       344\n",
      "\n",
      "    accuracy                           0.68       500\n",
      "   macro avg       0.14      0.20      0.16       500\n",
      "weighted avg       0.47      0.68      0.56       500\n",
      "\n",
      "\n",
      "multinomialClassification Report\n",
      "[[  0   0   0   0   2]\n",
      " [  0   0   0   0  10]\n",
      " [  0   0   0   0  42]\n",
      " [  1   1   0   0 100]\n",
      " [  2   0   0   0 342]]\n",
      "\n",
      " ovr Accuracy 0.688\n",
      "\n",
      "ovrClassification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         2\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.00      0.00      0.00        42\n",
      "           4       0.00      0.00      0.00       102\n",
      "           5       0.69      1.00      0.82       344\n",
      "\n",
      "    accuracy                           0.69       500\n",
      "   macro avg       0.14      0.20      0.16       500\n",
      "weighted avg       0.47      0.69      0.56       500\n",
      "\n",
      "\n",
      "ovrClassification Report\n",
      "[[  0   0   0   0   2]\n",
      " [  0   0   0   0  10]\n",
      " [  0   0   0   0  42]\n",
      " [  0   0   0   0 102]\n",
      " [  0   0   0   0 344]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loki/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n",
      "/home/loki/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for mclass in ('multinomial', 'ovr'):\n",
    "    lr = LogisticRegression(solver='lbfgs', max_iter=30000, random_state=0, multi_class=mclass).fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "     \n",
    "    # the 3 lines below show how to invoke various output    \n",
    "    print(\"\\n\",mclass,\"Accuracy\",accuracy_score(y_test, yhat))\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",classification_report(y_test, yhat),sep=\"\")\n",
    "    print(\"\\n\",mclass,\"Classification Report\\n\",confusion_matrix(y_test, yhat),sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
